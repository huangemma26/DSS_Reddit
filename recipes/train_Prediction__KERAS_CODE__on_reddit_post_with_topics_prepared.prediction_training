{
    "generatingModelId": "A-TOXICREDDIT-L5Q4nfEY-2QN5f0gd-s14-pp1-m1", 
    "partSource": "ACTIVE_VERSION", 
    "script": {
        "maxProcessedMemTableBytes": -1, 
        "sorting": [], 
        "columnsSelection": {
            "mode": "ALL"
        }, 
        "columnWidthsByName": {}, 
        "analysisColumnData": {}, 
        "exploreUIParams": {}, 
        "vizSampling": {
            "autoRefreshSample": false, 
            "_refreshTrigger": 0
        }, 
        "coloring": {
            "scheme": "MEANING_AND_STATUS", 
            "valueColoringMode": "HASH", 
            "individualColumns": []
        }, 
        "steps": [], 
        "globalSearchQuery": "", 
        "previewMode": "ALL_ROWS", 
        "explorationSampling": {
            "autoRefreshSample": false, 
            "selection": {
                "targetRatio": 0.02, 
                "maxReadUncompressedBytes": -1, 
                "ordering": {
                    "rules": [], 
                    "enabled": false
                }, 
                "latestPartitionsN": 1, 
                "filter": {
                    "distinct": false, 
                    "enabled": false
                }, 
                "withinFirstN": -1, 
                "maxRecords": 10000, 
                "partitionSelectionMethod": "ALL", 
                "maxStoredBytes": 104857600, 
                "samplingMethod": "HEAD_SEQUENTIAL"
            }, 
            "_refreshTrigger": 0
        }, 
        "explorationFilters": []
    }, 
    "splitParams": {
        "kfold": false, 
        "splitBeforePrepare": true, 
        "instanceIdRefresher": 0, 
        "ttPolicy": "SPLIT_SINGLE_DATASET", 
        "testOnLargerValues": true, 
        "ssdSplitMode": "RANDOM", 
        "ssdSelection": {
            "targetRatio": 0.02, 
            "maxReadUncompressedBytes": -1, 
            "ordering": {
                "rules": [], 
                "enabled": false
            }, 
            "latestPartitionsN": 1, 
            "filter": {
                "distinct": false, 
                "enabled": false
            }, 
            "withinFirstN": -1, 
            "maxRecords": 100000, 
            "partitionSelectionMethod": "ALL", 
            "useMemTable": false, 
            "samplingMethod": "HEAD_SEQUENTIAL"
        }, 
        "subSamplingSeed": 1337, 
        "ssdSeed": 1337, 
        "nFolds": 5, 
        "ssdTrainingRatio": 0.8
    }, 
    "backendType": "KERAS", 
    "envName": "Tensorflow", 
    "sparkParams": {
        "sparkUseGlobalMetastore": false, 
        "sparkExecutionEngine": "SPARK_SUBMIT", 
        "sparkPreparedDFStorageLevel": "MEMORY_AND_DISK", 
        "pipelineAllowStart": true, 
        "pipelineAllowMerge": true, 
        "sparkConf": {
            "inheritConf": "default", 
            "conf": []
        }, 
        "sparkRepartitionNonHDFS": 1
    }, 
    "preprocessing": {
        "preprocessingFitSampleSeed": 1337, 
        "feature_selection_params": {
            "custom_params": {
                "code": "# type your code here"
            }, 
            "pca_params": {
                "variance_proportion": 0.9, 
                "n_features": 25
            }, 
            "random_forest_params": {
                "depth": 10, 
                "n_features": 25, 
                "n_trees": 30
            }, 
            "lasso_params": {
                "alpha": [
                    0.01, 
                    0.1, 
                    1.0, 
                    10.0, 
                    100.0
                ], 
                "cross_validate": true
            }, 
            "method": "NONE", 
            "correlation_params": {
                "max_abs_correlation": 1.0, 
                "n_features": 25, 
                "min_abs_correlation": 0.0
            }
        }, 
        "preprocessingFitSampleRatio": 1.0, 
        "reduce": {
            "enabled": false, 
            "kept_variance": 0.0
        }, 
        "skipPreprocessing": false, 
        "target_remapping": [
            {
                "mappedValue": 0, 
                "sourceValue": "0", 
                "sampleFreq": 5979
            }, 
            {
                "mappedValue": 1, 
                "sourceValue": "1", 
                "sampleFreq": 3708
            }
        ], 
        "per_feature": {
            "sexist(sub)": {
                "generate_derivative": false, 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "sendToInput": "main", 
                "binarize_threshold_mode": "MEDIAN", 
                "state": {
                    "userModified": false, 
                    "recordedMeaning": "LongMeaning", 
                    "autoModifiedByDSS": false
                }, 
                "role": "TARGET", 
                "binarize_constant_threshold": 0.0, 
                "quantile_bin_nb_bins": 4, 
                "type": "NUMERIC", 
                "impute_constant_value": 0.0
            }, 
            "gilded": {
                "generate_derivative": false, 
                "numerical_handling": "REGULAR", 
                "missing_impute_with": "MEAN", 
                "role": "REJECT", 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "sendToInput": "main", 
                "binarize_threshold_mode": "MEDIAN", 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "LongMeaning", 
                    "autoModifiedByDSS": false
                }, 
                "missing_handling": "IMPUTE", 
                "binarize_constant_threshold": 0.0, 
                "quantile_bin_nb_bins": 4, 
                "rescaling": "AVGSTD", 
                "type": "NUMERIC", 
                "impute_constant_value": 0.0
            }, 
            "text_stemmed": {
                "hashSVDHashSize": 200000, 
                "ngramMinSize": 1, 
                "customHandlingCode": "from dataiku.doctor.deep_learning.preprocessing import TokenizerProcessor\n\n# Defines a processor that tokenizes a text. It computes a vocabulary on all the corpus.\n# Then, each text is converted to a vector representing the sequence of words, where each \n# element represents the index of the corresponding word in the vocabulary. The result is \n# padded with 0 up to the `max_len` in order for all the vectors to have the same length.\n\n#   num_words  - maximum number of words in the vocabulary\n#   max_len    - length of each sequence. If the text is longer,\n#                it will be truncated, and if it is shorter, it will be padded\n#                with 0.\nprocessor = TokenizerProcessor(num_words=10000, max_len=300)", 
                "hashSVDSVDLimit": 50000, 
                "customProcessorWantsMatrix": false, 
                "hashSVDSVDComponents": 100, 
                "sendToInput": "text_stemmed_preprocessed_1", 
                "maxWords": 0, 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "FreeText", 
                    "autoModifiedByDSS": false
                }, 
                "minRowsRatio": 0.001, 
                "maxRowsRatio": 0.8, 
                "useCustomVectorizer": false, 
                "stopWordsMode": "NONE", 
                "ngramMaxSize": 1, 
                "role": "INPUT", 
                "type": "TEXT", 
                "text_handling": "CUSTOM", 
                "name": "text_stemmed"
            }, 
            "title": {
                "hashSVDHashSize": 200000, 
                "ngramMinSize": 1, 
                "customHandlingCode": "from dataiku.doctor.deep_learning.preprocessing import TokenizerProcessor\n\n# Defines a processor that tokenizes a text. It computes a vocabulary on all the corpus.\n# Then, each text is converted to a vector representing the sequence of words, where each \n# element represents the index of the corresponding word in the vocabulary. The result is \n# padded with 0 up to the `max_len` in order for all the vectors to have the same length.\n\n#   num_words  - maximum number of words in the vocabulary\n#   max_len    - length of each sequence. If the text is longer,\n#                it will be truncated, and if it is shorter, it will be padded\n#                with 0.\nprocessor = TokenizerProcessor(num_words=10000, max_len=32)", 
                "hashSVDSVDLimit": 50000, 
                "customProcessorWantsMatrix": false, 
                "hashSVDSVDComponents": 100, 
                "sendToInput": "title_preprocessed", 
                "maxWords": 0, 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "FreeText", 
                    "autoModifiedByDSS": false
                }, 
                "minRowsRatio": 0.001, 
                "maxRowsRatio": 0.8, 
                "useCustomVectorizer": false, 
                "stopWordsMode": "NONE", 
                "ngramMaxSize": 1, 
                "role": "REJECT", 
                "type": "TEXT", 
                "text_handling": "CUSTOM", 
                "name": "title"
            }, 
            "num_comments": {
                "generate_derivative": false, 
                "numerical_handling": "REGULAR", 
                "missing_impute_with": "MEAN", 
                "role": "REJECT", 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "sendToInput": "main", 
                "binarize_threshold_mode": "MEDIAN", 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "LongMeaning", 
                    "autoModifiedByDSS": false
                }, 
                "missing_handling": "IMPUTE", 
                "binarize_constant_threshold": 0.0, 
                "quantile_bin_nb_bins": 4, 
                "rescaling": "AVGSTD", 
                "type": "NUMERIC", 
                "impute_constant_value": 0.0
            }, 
            "sexist(topic)": {
                "generate_derivative": false, 
                "numerical_handling": "REGULAR", 
                "missing_impute_with": "MEAN", 
                "role": "REJECT", 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "sendToInput": "main", 
                "binarize_threshold_mode": "MEDIAN", 
                "state": {
                    "userModified": false, 
                    "recordedMeaning": "LongMeaning", 
                    "autoModifiedByDSS": false
                }, 
                "missing_handling": "IMPUTE", 
                "binarize_constant_threshold": 0.0, 
                "quantile_bin_nb_bins": 4, 
                "rescaling": "AVGSTD", 
                "type": "NUMERIC", 
                "impute_constant_value": 0.0
            }, 
            "created_utc": {
                "generate_derivative": false, 
                "numerical_handling": "REGULAR", 
                "missing_impute_with": "MEAN", 
                "role": "REJECT", 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "sendToInput": "main", 
                "binarize_threshold_mode": "MEDIAN", 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "LongMeaning", 
                    "autoModifiedByDSS": false
                }, 
                "missing_handling": "IMPUTE", 
                "binarize_constant_threshold": 0.0, 
                "quantile_bin_nb_bins": 4, 
                "rescaling": "AVGSTD", 
                "type": "NUMERIC", 
                "impute_constant_value": 0.0
            }, 
            "subreddit": {
                "missing_impute_with": "MODE", 
                "dummy_drop": "NONE", 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "nb_bins_hashing": 1048576, 
                "sendToInput": "main", 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "Text", 
                    "autoModifiedByDSS": false
                }, 
                "role": "REJECT", 
                "max_nb_categories": 100, 
                "dummy_clip": "MAX_NB_CATEGORIES", 
                "missing_handling": "NONE", 
                "max_cat_safety": 200, 
                "min_samples": 10, 
                "type": "CATEGORY", 
                "cumulative_proportion": 0.95, 
                "category_handling": "DUMMIFY"
            }, 
            "lda_topic": {
                "generate_derivative": false, 
                "numerical_handling": "REGULAR", 
                "missing_impute_with": "MEAN", 
                "role": "REJECT", 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "sendToInput": "main", 
                "binarize_threshold_mode": "MEDIAN", 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "LongMeaning", 
                    "autoModifiedByDSS": false
                }, 
                "missing_handling": "IMPUTE", 
                "binarize_constant_threshold": 0.0, 
                "quantile_bin_nb_bins": 4, 
                "rescaling": "AVGSTD", 
                "type": "NUMERIC", 
                "impute_constant_value": 0.0
            }, 
            "score": {
                "generate_derivative": false, 
                "numerical_handling": "REGULAR", 
                "missing_impute_with": "MEAN", 
                "role": "REJECT", 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "sendToInput": "main", 
                "binarize_threshold_mode": "MEDIAN", 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "DoubleMeaning", 
                    "autoModifiedByDSS": false
                }, 
                "missing_handling": "IMPUTE", 
                "binarize_constant_threshold": 0.0, 
                "quantile_bin_nb_bins": 4, 
                "rescaling": "AVGSTD", 
                "type": "NUMERIC", 
                "impute_constant_value": 0.0
            }, 
            "retrieved_on": {
                "generate_derivative": false, 
                "numerical_handling": "REGULAR", 
                "missing_impute_with": "MEAN", 
                "role": "REJECT", 
                "customHandlingCode": "", 
                "customProcessorWantsMatrix": false, 
                "sendToInput": "main", 
                "binarize_threshold_mode": "MEDIAN", 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "LongMeaning", 
                    "autoModifiedByDSS": false
                }, 
                "missing_handling": "IMPUTE", 
                "binarize_constant_threshold": 0.0, 
                "quantile_bin_nb_bins": 4, 
                "rescaling": "AVGSTD", 
                "type": "NUMERIC", 
                "impute_constant_value": 0.0
            }, 
            "selftext": {
                "hashSVDHashSize": 200000, 
                "ngramMinSize": 1, 
                "customHandlingCode": "from dataiku.doctor.deep_learning.preprocessing import TokenizerProcessor\n\n# Defines a processor that tokenizes a text. It computes a vocabulary on all the corpus.\n# Then, each text is converted to a vector representing the sequence of words, where each \n# element represents the index of the corresponding word in the vocabulary. The result is \n# padded with 0 up to the `max_len` in order for all the vectors to have the same length.\n\n#   num_words  - maximum number of words in the vocabulary\n#   max_len    - length of each sequence. If the text is longer,\n#                it will be truncated, and if it is shorter, it will be padded\n#                with 0.\nprocessor = TokenizerProcessor(num_words=10000, max_len=32)", 
                "hashSVDSVDLimit": 50000, 
                "customProcessorWantsMatrix": false, 
                "hashSVDSVDComponents": 100, 
                "sendToInput": "selftext_preprocessed", 
                "maxWords": 0, 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "FreeText", 
                    "autoModifiedByDSS": false
                }, 
                "minRowsRatio": 0.001, 
                "maxRowsRatio": 0.8, 
                "useCustomVectorizer": false, 
                "stopWordsMode": "NONE", 
                "ngramMaxSize": 1, 
                "role": "REJECT", 
                "type": "TEXT", 
                "text_handling": "CUSTOM", 
                "name": "selftext"
            }, 
            "text": {
                "hashSVDHashSize": 200000, 
                "ngramMinSize": 1, 
                "customHandlingCode": "from dataiku.doctor.deep_learning.preprocessing import TokenizerProcessor\n\n# Defines a processor that tokenizes a text. It computes a vocabulary on all the corpus.\n# Then, each text is converted to a vector representing the sequence of words, where each \n# element represents the index of the corresponding word in the vocabulary. The result is \n# padded with 0 up to the `max_len` in order for all the vectors to have the same length.\n\n#   num_words  - maximum number of words in the vocabulary\n#   max_len    - length of each sequence. If the text is longer,\n#                it will be truncated, and if it is shorter, it will be padded\n#                with 0.\nprocessor = TokenizerProcessor(num_words=10000, max_len=32)", 
                "hashSVDSVDLimit": 50000, 
                "customProcessorWantsMatrix": false, 
                "hashSVDSVDComponents": 100, 
                "sendToInput": "text_preprocessed", 
                "maxWords": 0, 
                "state": {
                    "userModified": true, 
                    "recordedMeaning": "FreeText", 
                    "autoModifiedByDSS": false
                }, 
                "minRowsRatio": 0.001, 
                "maxRowsRatio": 0.8, 
                "useCustomVectorizer": false, 
                "stopWordsMode": "NONE", 
                "ngramMaxSize": 1, 
                "role": "REJECT", 
                "type": "TEXT", 
                "text_handling": "CUSTOM", 
                "name": "text"
            }
        }, 
        "feature_generation": {
            "manual_interactions": {
                "interactions": []
            }, 
            "pairwise_linear": {
                "behavior": "DISABLED"
            }, 
            "categoricals_count_transformer": {
                "input_features": [], 
                "all_features": false, 
                "behavior": "DISABLED"
            }, 
            "polynomial_combinations": {
                "behavior": "DISABLED"
            }, 
            "numericals_clustering": {
                "k": 0, 
                "input_features": [], 
                "all_features": false, 
                "behavior": "DISABLED"
            }
        }
    }, 
    "envSelection": {
        "envName": "Tensorflow", 
        "envMode": "EXPLICIT_ENV"
    }, 
    "modeling": {
        "metrics": {
            "customEvaluationMetricGIB": true, 
            "customEvaluationMetricNeedsProba": false, 
            "thresholdOptimizationMetric": "F1", 
            "evaluationMetric": "ROC_AUC", 
            "costMatrixWeights": {
                "fnGain": 0.0, 
                "tpGain": 1.0, 
                "fpGain": -0.3, 
                "tnGain": 0.0
            }, 
            "liftPoint": 0.4
        }, 
        "algorithm": "KERAS_CODE", 
        "keras": {
            "perGPUMemoryFraction": 0.5, 
            "trainOnAllData": true, 
            "useGPU": false, 
            "stepsPerEpoch": 100, 
            "batchSize": 256, 
            "enabled": true, 
            "gpuAllowGrowth": false, 
            "epochs": 10, 
            "kerasInputs": [
                "main", 
                "text_preprocessed"
            ], 
            "advancedFitMode": false, 
            "gpuList": [
                0
            ], 
            "shuffleData": true, 
            "buildCode": "## PREVIOUS CODE\nfrom keras.layers import Embedding, LSTM\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.models import Model\n\ndef build_model(input_shapes, n_classes=None):\n\n    #### DEFINING THE INPUT\n    # You need to modify the name and length of the \"text_input\" \n    # according to the preprocessing and name of your \n    # initial feature.\n    # This feature should to be preprocessed as a \"Text\", with a \n    # custom preprocessing using the \"TokenizerProcessor\" class\n    text_length = 300\n    vocabulary_size = 10000\n    text_input_name = \"text_stemmed_preprocessed_1\"\n\n    text_input = Input(shape=(text_length,), name=text_input_name)\n\n    #### DEFINING THE ARCHITECTURE\n    emb = Embedding(output_dim=512, input_dim=vocabulary_size, input_length=text_length)(text_input)\n    lstm_out = LSTM(128)(emb)\n    \n    x = Dense(128, activation='relu')(lstm_out)\n    x = Dense(64, activation='relu')(x)\n    predictions = Dense(n_classes, activation='softmax')(x)\n\n    model = Model(inputs=text_input, outputs=predictions)\n\n    return model\n\ndef compile_model(model):\n    model.compile(\n        optimizer=\"adam\",\n        loss=\"categorical_crossentropy\"\n    )\n    return model\n\n"
        }, 
        "grid_search_params": {
            "nIter": 0, 
            "splitRatio": 0.8, 
            "strategy": "GRID", 
            "randomized": true, 
            "shuffleIterations": 1, 
            "mode": "KFOLD", 
            "timeout": 0, 
            "nJobs": 4, 
            "nFolds": 3, 
            "stratified": true
        }, 
        "autoOptimizeThreshold": true, 
        "gridLength": 10, 
        "computeLearningCurves": false, 
        "forcedClassifierThreshold": 0.0, 
        "skipExpensiveReports": false, 
        "max_ensemble_nodes_serialized": 0, 
        "pluginAlgoCustomGridSearch": false
    }, 
    "core": {
        "prediction_type": "BINARY_CLASSIFICATION", 
        "weight": {
            "weightMethod": "NO_WEIGHTING"
        }, 
        "calibration": {
            "calibrationMethod": "NO_CALIBRATION"
        }, 
        "partitionedModel": {
            "enabled": false, 
            "ssdSelection": {
                "targetRatio": 0.02, 
                "maxReadUncompressedBytes": -1, 
                "ordering": {
                    "rules": [], 
                    "enabled": false
                }, 
                "latestPartitionsN": 1, 
                "filter": {
                    "distinct": false, 
                    "enabled": false
                }, 
                "withinFirstN": -1, 
                "maxRecords": -1, 
                "partitionSelectionMethod": "ALL", 
                "useMemTable": false, 
                "samplingMethod": "FULL"
            }
        }, 
        "backendType": "KERAS", 
        "time": {
            "enabled": false, 
            "ascending": true
        }, 
        "executionParams": {
            "envName": "Tensorflow", 
            "sparkCheckpoint": "NONE", 
            "containerSelection": {
                "containerMode": "INHERIT"
            }, 
            "sparkParams": {
                "sparkUseGlobalMetastore": false, 
                "sparkExecutionEngine": "SPARK_SUBMIT", 
                "sparkPreparedDFStorageLevel": "MEMORY_AND_DISK", 
                "pipelineAllowStart": true, 
                "pipelineAllowMerge": true, 
                "sparkConf": {
                    "inheritConf": "default", 
                    "conf": []
                }, 
                "sparkRepartitionNonHDFS": 1
            }, 
            "envSelection": {
                "envName": "Tensorflow", 
                "envMode": "EXPLICIT_ENV"
            }
        }, 
        "taskType": "PREDICTION", 
        "target_variable": "sexist(sub)"
    }, 
    "expectedPreparationOutputSchema": {
        "userModified": false, 
        "columns": [
            {
                "type": "bigint", 
                "name": "created_utc"
            }, 
            {
                "type": "string", 
                "name": "subreddit"
            }, 
            {
                "type": "bigint", 
                "name": "sexist(sub)"
            }, 
            {
                "type": "bigint", 
                "name": "num_comments"
            }, 
            {
                "type": "double", 
                "name": "score"
            }, 
            {
                "type": "string", 
                "name": "title"
            }, 
            {
                "type": "string", 
                "name": "selftext"
            }, 
            {
                "type": "string", 
                "name": "text"
            }, 
            {
                "type": "string", 
                "name": "text_stemmed"
            }, 
            {
                "type": "bigint", 
                "name": "gilded"
            }, 
            {
                "type": "bigint", 
                "name": "retrieved_on"
            }, 
            {
                "type": "bigint", 
                "name": "lda_topic"
            }, 
            {
                "type": "bigint", 
                "name": "sexist(topic)"
            }
        ]
    }, 
    "operationMode": "TRAIN_SPLITTED_ONLY"
}